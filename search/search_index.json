{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"mack mack provides a variety of helper methods that make it easy for you to perform common Delta Lake operations. Setup Install mack with pip install mack . Here's an example of how you can perform a Type 2 SCD upsert with a single line of code using Mack: import mack mack.type_2_scd_upsert(path, updatesDF, \"pkey\", [\"attr1\", \"attr2\"]) Type 2 SCD Upserts This library provides an opinionated, conventions over configuration, approach to Type 2 SCD management. Let's look at an example before covering the conventions required to take advantage of the functionality. Suppose you have the following SCD table with the pkey primary key: +----+-----+-----+----------+-------------------+--------+ |pkey|attr1|attr2|is_current| effective_time|end_time| +----+-----+-----+----------+-------------------+--------+ | 1| A| A| true|2019-01-01 00:00:00| null| | 2| B| B| true|2019-01-01 00:00:00| null| | 4| D| D| true|2019-01-01 00:00:00| null| +----+-----+-----+----------+-------------------+--------+ You'd like to perform an upsert with this data: +----+-----+-----+-------------------+ |pkey|attr1|attr2| effective_time| +----+-----+-----+-------------------+ | 2| Z| null|2020-01-01 00:00:00| // upsert data | 3| C| C|2020-09-15 00:00:00| // new pkey +----+-----+-----+-------------------+ Here's how to perform the upsert: mack.type_2_scd_upsert(delta_table, updatesDF, \"pkey\", [\"attr1\", \"attr2\"]) Here's the table after the upsert: +----+-----+-----+----------+-------------------+-------------------+ |pkey|attr1|attr2|is_current| effective_time| end_time| +----+-----+-----+----------+-------------------+-------------------+ | 2| B| B| false|2019-01-01 00:00:00|2020-01-01 00:00:00| | 4| D| D| true|2019-01-01 00:00:00| null| | 1| A| A| true|2019-01-01 00:00:00| null| | 3| C| C| true|2020-09-15 00:00:00| null| | 2| Z| null| true|2020-01-01 00:00:00| null| +----+-----+-----+----------+-------------------+-------------------+ You can leverage the upsert code if your SCD table meets these requirements: Contains a unique primary key column Any change in an attribute column triggers an upsert SCD logic is exposed via effective_time , end_time and is_current column (you can also use date or version columns for SCD upserts) Kill duplicates The kill_duplicate function completely removes all duplicate rows from a Delta table. Suppose you have the following table: +----+----+----+ |col1|col2|col3| +----+----+----+ | 1| A| A| # duplicate | 2| A| B| | 3| A| A| # duplicate | 4| A| A| # duplicate | 5| B| B| # duplicate | 6| D| D| | 9| B| B| # duplicate +----+----+----+ Run the kill_duplicates function: mack.kill_duplicates(deltaTable, [\"col2\", \"col3\"]) Here's the ending state of the table: +----+----+----+ |col1|col2|col3| +----+----+----+ | 2| A| B| | 6| D| D| +----+----+----+ Drop duplicates with Primary Key The drop_duplicates_pkey function removes all but one duplicate row from a Delta table. Warning: You have to provide a primary column that must contain unique values , otherwise the method will default to kill the duplicates. If you can not provide a unique primary key, you can use the drop_duplicates method. Suppose you have the following table: +----+----+----+----+ |col1|col2|col3|col4| +----+----+----+----+ | 1| A| A| C| # duplicate1 | 2| A| B| C| | 3| A| A| D| # duplicate1 | 4| A| A| E| # duplicate1 | 5| B| B| C| # duplicate2 | 6| D| D| C| | 9| B| B| E| # duplicate2 +----+----+----+----+ Run the drop_duplicates function: mack.drop_duplicates_pkey(delta_table=deltaTable, primary_key=\"col1\", duplication_columns=[\"col2\", \"col3\"]) Here's the ending state of the table: +----+----+----+----+ |col1|col2|col3|col4| +----+----+----+----+ | 1| A| A| C| | 2| A| B| C| | 5| B| B| C| | 6| D| D| C| +----+----+----+----+ Drop duplicates The drop_duplicates function removes all but one duplicate row from a Delta table. It behaves exactly like the drop_duplicates DataFrame API. Warning: This method is overwriting the whole table, thus very inefficient. If you can, use the drop_duplicates_pkey method instead. Suppose you have the following table: +----+----+----+----+ |col1|col2|col3|col4| +----+----+----+----+ | 1| A| A| C| # duplicate | 1| A| A| C| # duplicate | 2| A| A| C| +----+----+----+----+ Run the drop_duplicates function: mack.drop_duplicates(delta_table=deltaTable, duplication_columns=[\"col1\"]) Here's the ending state of the table: +----+----+----+----+ |col1|col2|col3|col4| +----+----+----+----+ | 1| A| A| C| # duplicate | 2| A| A| C| # duplicate +----+----+----+----+ Copy table The copy_table function copies an existing Delta table. When you copy a table, it gets recreated at a specified target. This target could be a path or a table in a metastore. Copying includes: Data Partitioning Table properties Copying does not include the delta log, which means that you will not be able to restore the new table to an old version of the original table. Here's how to perform the copy: mack.copy_table(delta_table=deltaTable, target_path=path) Validate append The validate_append function provides a mechanism for allowing some columns for schema evolution, but rejecting appends with columns that aren't specificly allowlisted. Suppose you have the following Delta table: +----+----+----+ |col1|col2|col3| +----+----+----+ | 2| b| B| | 1| a| A| +----+----+----+ Here's a appender function that wraps validate_append : def append_fun(delta_table, append_df): mack.validate_append( delta_table, append_df, required_cols=[\"col1\", \"col2\"], optional_cols=[\"col4\"], ) You can append the following DataFrame that contains the required columns and the optional columns: +----+----+----+ |col1|col2|col4| +----+----+----+ | 3| c| cat| | 4| d| dog| +----+----+----+ Here's what the Delta table will contain after that data is appended: +----+----+----+----+ |col1|col2|col3|col4| +----+----+----+----+ | 3| c|null| cat| | 4| d|null| dog| | 2| b| B|null| | 1| a| A|null| +----+----+----+----+ You cannot append the following DataFrame which contains the required columns, but also contains another column ( col5 ) that's not specified as an optional column. +----+----+----+ |col1|col2|col5| +----+----+----+ | 4| b| A| | 5| y| C| | 6| z| D| +----+----+----+ Here's the error you'll get when you attempt this write: \"TypeError: The column 'col5' is not part of the current Delta table. If you want to add the column to the table you must set the optional_cols parameter.\" You also cannot append the following DataFrame which is missing one of the required columns. +----+----+ |col1|col4| +----+----+ | 4| A| | 5| C| | 6| D| +----+----+ Here's the error you'll get: \"TypeError: The base Delta table has these columns '['col1', 'col4']', but these columns are required '['col1', 'col2']'.\" Append data without duplicates The append_without_duplicates function helps to append records to a existing Delta table without getting duplicates appended to the record. Suppose you have the following Delta table: +----+----+----+ |col1|col2|col3| +----+----+----+ | 1| A| B| | 2| C| D| | 3| E| F| +----+----+----+ Here is data to be appended: +----+----+----+ |col1|col2|col3| +----+----+----+ | 2| R| T| # duplicate col1 | 8| A| B| | 8| C| D| # duplicate col1 | 10| X| Y| +----+----+----+ Run the append_without_duplicates function: mack.append_without_duplicates(deltaTable, append_df, [\"col1\"]) Here's the ending result: +----+----+----+ |col1|col2|col3| +----+----+----+ | 1| A| B| | 2| C| D| | 3| E| F| | 8| A| B| | 10| X| Y| +----+----+----+ Notice that the duplicate col1 value was not appended. If a normal append operation was run, then the Delta table would contain two rows of data with col1 equal to 2. Delta File Sizes The delta_file_sizes function returns a dictionary that contains the total size in bytes, the amount of files and the average file size for a given Delta Table. Suppose you have the following Delta Table, partitioned by col1 : +----+----+----+ |col1|col2|col3| +----+----+----+ | 1| A| A| | 2| A| B| +----+----+----+ Running mack.delta_file_sizes(delta_table) on that table will return: {\"size_in_bytes\": 1320, \"number_of_files\": 2, \"average_file_size_in_bytes\": 660} Show Delta File Sizes The show_delta_file_sizes function prints the amount of files, the size of the table, and the average file size for a delta table. Suppose you have the following Delta Table, partitioned by col1 : +----+----+----+ |col1|col2|col3| +----+----+----+ | 1| A| A| | 2| A| B| +----+----+----+ Running mack.delta_file_sizes(delta_table) on that table will print: The delta table contains 2 files with a size of 1.32 kB. The average file size is 660.0 B Humanize Bytes The humanize_bytes function formats an integer representing a number of bytes in an easily human readable format. mack.humanize_bytes(1234567890) # \"1.23 GB\" mack.humanize_bytes(1234567890000) # \"1.23 TB\" It's a lot easier for a human to understand 1.23 GB compared to 1234567890 bytes. Is Composite Key Candidate The is_composite_key_candidate function returns a boolean that indicates whether a set of columns are unique and could form a composite key or not. Suppose you have the following Delta Table: +----+----+----+ |col1|col2|col3| +----+----+----+ | 1| A| A| | 2| B| B| | 2| C| B| +----+----+----+ Running mack.is_composite_key_candidate(delta_table, [\"col1\"]) on that table will return False . Running mack.is_composite_key_candidate(delta_table, [\"col1\", \"col2\"]) on that table will return True . Find Composite Key Candidates in the Delta table The find_composite_key_candidates function helps you find a composite key that uniquely identifies the rows your Delta table. It returns a list of columns that can be used as a composite key. Suppose you have the following Delta table: +----+----+----+ |col1|col2|col3| +----+----+----+ | 1| a| z| | 1| a| b| | 3| c| b| +----+----+----+ Running mack.find_composite_key_candidates(delta_table) on that table will return [\"col1\", \"col3\"] . Append md5 column The with_md5_cols function appends a md5 hash of specified columns to the DataFrame. This can be used as a unique key if the selected columns form a composite key. You can use this function with the columns identified in find_composite_key_candidates to append a unique key to the DataFrame. Suppose you have the following Delta table: +----+----+----+ |col1|col2|col3| +----+----+----+ | 1| a|null| | 2| b| b| | 3| c| c| +----+----+----+ Running mack.with_md5_cols(delta_table, [\"col2\", \"col3\"]) on that table will append a md5_col2_col3 as follows: +----+----+----+--------------------------------+ |col1|col2|col3|md5_col2_col3 | +----+----+----+--------------------------------+ |1 |a |null|0cc175b9c0f1b6a831c399e269772661| |2 |b |b |1eeaac3814eb80cc40efb005cf0b9141| |3 |c |c |4e202f8309e7b00349c70845ab02fce9| +----+----+----+--------------------------------+ Get Latest Delta Table Version The latest_version function gets the most current Delta Table version number and returns it. delta_table = DeltaTable.forPath(spark, path) mack.latest_version(delta_table) >> 2 Append data with constraints The constraint_append function helps to append records to an existing Delta table even if there are records in the append dataframe that violate table constraints (both check and not null constraints), these records are appended to an existing quarantine Delta table instead of the target table. If the quarantine Delta table is set to None , those records that violate table constraints are simply thrown out. Suppose you have the following target Delta table with the following schema and constraints: schema: col1 int not null col2 string null col3 string null check constraints: col1_constraint: (col1 > 0) col2_constraint: (col2 != 'Z') +----+----+----+ |col1|col2|col3| +----+----+----+ | 1| A| B| | 2| C| D| | 3| E| F| +----+----+----+ Suppose you have a quarantine Delta table with the same schema but without the constraints. Here is data to be appended: +----+----+----+ |col1|col2|col3| +----+----+----+ | | H| H| # violates col1 not null constraint | 0| Z| Z| # violates both col1_constraint and col2_constraint | 4| A| B| | 5| C| D| | 6| E| F| | 9| G| G| | 11| Z| Z| # violates col2_constraint +----+----+----+ Run the constraint_append function: mack.constraint_append(delta_table, append_df, quarantine_table) Here's the ending result in delta_table: +----+----+----+ |col1|col2|col3| +----+----+----+ | 1| A| B| | 2| C| D| | 3| E| F| | 4| A| B| | 5| C| D| | 6| E| F| | 9| G| G| +----+----+----+ Here's the ending result in quarantine_table: +----+----+----+ |col1|col2|col3| +----+----+----+ | | H| H| | 0| Z| Z| | 11| Z| Z| +----+----+----+ Notice that the records that violated either of the constraints are appended to the quarantine table all other records are appended to the target table and the append has not failed. If a normal append operation was run, then it would have failed on the constraint violation. If quarantine_table is set to None , records that violated either of the constraints are simply thrown out. Rename a Delta Table This function is designed to rename a Delta table. It can operate either within a Databricks environment or with a standalone Spark session. Parameters: delta_table ( DeltaTable ): An object representing the Delta table to be renamed. new_table_name ( str ): The new name for the table. table_location ( str , optional): The file path where the table is stored. If not provided, the function attempts to deduce the location from the DeltaTable object. Defaults to None . databricks ( bool , optional): A flag indicating the function's operational environment. Set to True if running within Databricks, otherwise, False . Defaults to False . spark_session ( pyspark.sql.SparkSession , optional): The Spark session. This is required when databricks is set to True . Defaults to None . Returns: None Raises: TypeError : If the provided delta_table is not a DeltaTable object, or if databricks is set to True and spark_session is None . Example Usage: rename_delta_table(existing_delta_table, \"new_table_name\") Dictionary We're leveraging the following terminology defined here . Natural key: an attribute that can uniquely identify a row, and exists in the real world. Surrogate key: an attribute that can uniquely identify a row, and does not exist in the real world. Composite key: more than one attribute that when combined can uniquely identify a row. Primary key: the single unique identifier for the row. Candidate key: an attribute that could be the primary key. Alternate key: a candidate key that is not the primary key. Unique key: an attribute that can be unique on the table. Can also be called an alternate key. Foreign key: an attribute that is used to refer to another record in another table. Project maintainers Matthew Powers aka MrPowers Robert Kossendey aka robertkossendey Souvik Pratiher aka souvik-databricks Project philosophy The mack library is designed to make common Delta Lake data tasks easier. You don't need to use mack of course. You can write the logic yourself. If you don't want to add a dependency to your project, you can also easily copy / paste the functions from mack. The functions in this library are intentionally designed to be easy to copy and paste. Let's look at some of the reasons you may want to add mack as a dependency. Exposing nice public interfaces The public interface (and only the public interface) is available via the mack namespace. When you run import mack , you can access the entirety of the public interface. No private implementation details are exposed in the mack namespace. Minimal dependencies Mack only depends on Spark & Delta Lake. No other dependencies will be added to Mack. Spark users leverage a variety of runtimes and it's not always easy to add a dependency. You can run pip install mack and won't have to worry about resolving a lot of dependency conflicts. You can also Just attach a mack wheel file to a cluster to leverage the project. Provide best practices examples for the community Mack strives to be a good example codebase for the PySpark / Delta Lake community. There aren't a lot of open source Delta Lake projects. There are even fewer that use good software engineering practices like CI and unit testing. You can use mack to help guide your design decisions in proprietary code repos. Stable public interfaces and long term support after 1.0 release Mack reserves the right to make breaking public interface changes before the 1.0 release. We'll always minimize breaking changes whenever possible. After the 1.0 release, Mack will stricly follow Semantic Versioning 2.0 and will only make breaking public interface changes in major releases. Hopefully 1.0 will be the only major release and there won't have to be any breaking changes. Code design Here are some of the code design principles used in Mack: We avoid classes whenever possible. Classes make it harder to copy / paste little chunks of code into notebooks. It's good to Stop Writing Classes . We try to make functions that are easy to copy. We do this by limiting functions that depend on other functions or classes. We'd rather nest a single use function in a public interface method than make it separate. Develop and then abstract. All code goes in a single file till the right abstractions become apparent. We'd rather have a large file than the wrong abstractions. Docker Environment The Dockerfile and docker-compose files provide a containerized way to run and develop with mack . The first time run docker build --tag=mack . to build the image. To execute the unit tests inside the Docker container, run docker-compose up test To drop into the running Docker container to develop, run docker run -it mack /bin/bash Community Blogs Daniel Beach (Confessions of a Data Guy): Simplify Delta Lake Complexity with mack. Bartosz Konieczny (waitingforcode): Simplified Delta Lake operations with Mack Videos GeekCoders on YouTube: How I use MACK Library in Delta Lake using Databricks/PySpark","title":"Mack"},{"location":"#mack","text":"mack provides a variety of helper methods that make it easy for you to perform common Delta Lake operations.","title":"mack"},{"location":"#setup","text":"Install mack with pip install mack . Here's an example of how you can perform a Type 2 SCD upsert with a single line of code using Mack: import mack mack.type_2_scd_upsert(path, updatesDF, \"pkey\", [\"attr1\", \"attr2\"])","title":"Setup"},{"location":"#type-2-scd-upserts","text":"This library provides an opinionated, conventions over configuration, approach to Type 2 SCD management. Let's look at an example before covering the conventions required to take advantage of the functionality. Suppose you have the following SCD table with the pkey primary key: +----+-----+-----+----------+-------------------+--------+ |pkey|attr1|attr2|is_current| effective_time|end_time| +----+-----+-----+----------+-------------------+--------+ | 1| A| A| true|2019-01-01 00:00:00| null| | 2| B| B| true|2019-01-01 00:00:00| null| | 4| D| D| true|2019-01-01 00:00:00| null| +----+-----+-----+----------+-------------------+--------+ You'd like to perform an upsert with this data: +----+-----+-----+-------------------+ |pkey|attr1|attr2| effective_time| +----+-----+-----+-------------------+ | 2| Z| null|2020-01-01 00:00:00| // upsert data | 3| C| C|2020-09-15 00:00:00| // new pkey +----+-----+-----+-------------------+ Here's how to perform the upsert: mack.type_2_scd_upsert(delta_table, updatesDF, \"pkey\", [\"attr1\", \"attr2\"]) Here's the table after the upsert: +----+-----+-----+----------+-------------------+-------------------+ |pkey|attr1|attr2|is_current| effective_time| end_time| +----+-----+-----+----------+-------------------+-------------------+ | 2| B| B| false|2019-01-01 00:00:00|2020-01-01 00:00:00| | 4| D| D| true|2019-01-01 00:00:00| null| | 1| A| A| true|2019-01-01 00:00:00| null| | 3| C| C| true|2020-09-15 00:00:00| null| | 2| Z| null| true|2020-01-01 00:00:00| null| +----+-----+-----+----------+-------------------+-------------------+ You can leverage the upsert code if your SCD table meets these requirements: Contains a unique primary key column Any change in an attribute column triggers an upsert SCD logic is exposed via effective_time , end_time and is_current column (you can also use date or version columns for SCD upserts)","title":"Type 2 SCD Upserts"},{"location":"#kill-duplicates","text":"The kill_duplicate function completely removes all duplicate rows from a Delta table. Suppose you have the following table: +----+----+----+ |col1|col2|col3| +----+----+----+ | 1| A| A| # duplicate | 2| A| B| | 3| A| A| # duplicate | 4| A| A| # duplicate | 5| B| B| # duplicate | 6| D| D| | 9| B| B| # duplicate +----+----+----+ Run the kill_duplicates function: mack.kill_duplicates(deltaTable, [\"col2\", \"col3\"]) Here's the ending state of the table: +----+----+----+ |col1|col2|col3| +----+----+----+ | 2| A| B| | 6| D| D| +----+----+----+","title":"Kill duplicates"},{"location":"#drop-duplicates-with-primary-key","text":"The drop_duplicates_pkey function removes all but one duplicate row from a Delta table. Warning: You have to provide a primary column that must contain unique values , otherwise the method will default to kill the duplicates. If you can not provide a unique primary key, you can use the drop_duplicates method. Suppose you have the following table: +----+----+----+----+ |col1|col2|col3|col4| +----+----+----+----+ | 1| A| A| C| # duplicate1 | 2| A| B| C| | 3| A| A| D| # duplicate1 | 4| A| A| E| # duplicate1 | 5| B| B| C| # duplicate2 | 6| D| D| C| | 9| B| B| E| # duplicate2 +----+----+----+----+ Run the drop_duplicates function: mack.drop_duplicates_pkey(delta_table=deltaTable, primary_key=\"col1\", duplication_columns=[\"col2\", \"col3\"]) Here's the ending state of the table: +----+----+----+----+ |col1|col2|col3|col4| +----+----+----+----+ | 1| A| A| C| | 2| A| B| C| | 5| B| B| C| | 6| D| D| C| +----+----+----+----+","title":"Drop duplicates with Primary Key"},{"location":"#drop-duplicates","text":"The drop_duplicates function removes all but one duplicate row from a Delta table. It behaves exactly like the drop_duplicates DataFrame API. Warning: This method is overwriting the whole table, thus very inefficient. If you can, use the drop_duplicates_pkey method instead. Suppose you have the following table: +----+----+----+----+ |col1|col2|col3|col4| +----+----+----+----+ | 1| A| A| C| # duplicate | 1| A| A| C| # duplicate | 2| A| A| C| +----+----+----+----+ Run the drop_duplicates function: mack.drop_duplicates(delta_table=deltaTable, duplication_columns=[\"col1\"]) Here's the ending state of the table: +----+----+----+----+ |col1|col2|col3|col4| +----+----+----+----+ | 1| A| A| C| # duplicate | 2| A| A| C| # duplicate +----+----+----+----+","title":"Drop duplicates"},{"location":"#copy-table","text":"The copy_table function copies an existing Delta table. When you copy a table, it gets recreated at a specified target. This target could be a path or a table in a metastore. Copying includes: Data Partitioning Table properties Copying does not include the delta log, which means that you will not be able to restore the new table to an old version of the original table. Here's how to perform the copy: mack.copy_table(delta_table=deltaTable, target_path=path)","title":"Copy table"},{"location":"#validate-append","text":"The validate_append function provides a mechanism for allowing some columns for schema evolution, but rejecting appends with columns that aren't specificly allowlisted. Suppose you have the following Delta table: +----+----+----+ |col1|col2|col3| +----+----+----+ | 2| b| B| | 1| a| A| +----+----+----+ Here's a appender function that wraps validate_append : def append_fun(delta_table, append_df): mack.validate_append( delta_table, append_df, required_cols=[\"col1\", \"col2\"], optional_cols=[\"col4\"], ) You can append the following DataFrame that contains the required columns and the optional columns: +----+----+----+ |col1|col2|col4| +----+----+----+ | 3| c| cat| | 4| d| dog| +----+----+----+ Here's what the Delta table will contain after that data is appended: +----+----+----+----+ |col1|col2|col3|col4| +----+----+----+----+ | 3| c|null| cat| | 4| d|null| dog| | 2| b| B|null| | 1| a| A|null| +----+----+----+----+ You cannot append the following DataFrame which contains the required columns, but also contains another column ( col5 ) that's not specified as an optional column. +----+----+----+ |col1|col2|col5| +----+----+----+ | 4| b| A| | 5| y| C| | 6| z| D| +----+----+----+ Here's the error you'll get when you attempt this write: \"TypeError: The column 'col5' is not part of the current Delta table. If you want to add the column to the table you must set the optional_cols parameter.\" You also cannot append the following DataFrame which is missing one of the required columns. +----+----+ |col1|col4| +----+----+ | 4| A| | 5| C| | 6| D| +----+----+ Here's the error you'll get: \"TypeError: The base Delta table has these columns '['col1', 'col4']', but these columns are required '['col1', 'col2']'.\"","title":"Validate append"},{"location":"#append-data-without-duplicates","text":"The append_without_duplicates function helps to append records to a existing Delta table without getting duplicates appended to the record. Suppose you have the following Delta table: +----+----+----+ |col1|col2|col3| +----+----+----+ | 1| A| B| | 2| C| D| | 3| E| F| +----+----+----+ Here is data to be appended: +----+----+----+ |col1|col2|col3| +----+----+----+ | 2| R| T| # duplicate col1 | 8| A| B| | 8| C| D| # duplicate col1 | 10| X| Y| +----+----+----+ Run the append_without_duplicates function: mack.append_without_duplicates(deltaTable, append_df, [\"col1\"]) Here's the ending result: +----+----+----+ |col1|col2|col3| +----+----+----+ | 1| A| B| | 2| C| D| | 3| E| F| | 8| A| B| | 10| X| Y| +----+----+----+ Notice that the duplicate col1 value was not appended. If a normal append operation was run, then the Delta table would contain two rows of data with col1 equal to 2.","title":"Append data without duplicates"},{"location":"#delta-file-sizes","text":"The delta_file_sizes function returns a dictionary that contains the total size in bytes, the amount of files and the average file size for a given Delta Table. Suppose you have the following Delta Table, partitioned by col1 : +----+----+----+ |col1|col2|col3| +----+----+----+ | 1| A| A| | 2| A| B| +----+----+----+ Running mack.delta_file_sizes(delta_table) on that table will return: {\"size_in_bytes\": 1320, \"number_of_files\": 2, \"average_file_size_in_bytes\": 660}","title":"Delta File Sizes"},{"location":"#show-delta-file-sizes","text":"The show_delta_file_sizes function prints the amount of files, the size of the table, and the average file size for a delta table. Suppose you have the following Delta Table, partitioned by col1 : +----+----+----+ |col1|col2|col3| +----+----+----+ | 1| A| A| | 2| A| B| +----+----+----+ Running mack.delta_file_sizes(delta_table) on that table will print: The delta table contains 2 files with a size of 1.32 kB. The average file size is 660.0 B","title":"Show Delta File Sizes"},{"location":"#humanize-bytes","text":"The humanize_bytes function formats an integer representing a number of bytes in an easily human readable format. mack.humanize_bytes(1234567890) # \"1.23 GB\" mack.humanize_bytes(1234567890000) # \"1.23 TB\" It's a lot easier for a human to understand 1.23 GB compared to 1234567890 bytes.","title":"Humanize Bytes"},{"location":"#is-composite-key-candidate","text":"The is_composite_key_candidate function returns a boolean that indicates whether a set of columns are unique and could form a composite key or not. Suppose you have the following Delta Table: +----+----+----+ |col1|col2|col3| +----+----+----+ | 1| A| A| | 2| B| B| | 2| C| B| +----+----+----+ Running mack.is_composite_key_candidate(delta_table, [\"col1\"]) on that table will return False . Running mack.is_composite_key_candidate(delta_table, [\"col1\", \"col2\"]) on that table will return True .","title":"Is Composite Key Candidate"},{"location":"#find-composite-key-candidates-in-the-delta-table","text":"The find_composite_key_candidates function helps you find a composite key that uniquely identifies the rows your Delta table. It returns a list of columns that can be used as a composite key. Suppose you have the following Delta table: +----+----+----+ |col1|col2|col3| +----+----+----+ | 1| a| z| | 1| a| b| | 3| c| b| +----+----+----+ Running mack.find_composite_key_candidates(delta_table) on that table will return [\"col1\", \"col3\"] .","title":"Find Composite Key Candidates in the Delta table"},{"location":"#append-md5-column","text":"The with_md5_cols function appends a md5 hash of specified columns to the DataFrame. This can be used as a unique key if the selected columns form a composite key. You can use this function with the columns identified in find_composite_key_candidates to append a unique key to the DataFrame. Suppose you have the following Delta table: +----+----+----+ |col1|col2|col3| +----+----+----+ | 1| a|null| | 2| b| b| | 3| c| c| +----+----+----+ Running mack.with_md5_cols(delta_table, [\"col2\", \"col3\"]) on that table will append a md5_col2_col3 as follows: +----+----+----+--------------------------------+ |col1|col2|col3|md5_col2_col3 | +----+----+----+--------------------------------+ |1 |a |null|0cc175b9c0f1b6a831c399e269772661| |2 |b |b |1eeaac3814eb80cc40efb005cf0b9141| |3 |c |c |4e202f8309e7b00349c70845ab02fce9| +----+----+----+--------------------------------+","title":"Append md5 column"},{"location":"#get-latest-delta-table-version","text":"The latest_version function gets the most current Delta Table version number and returns it. delta_table = DeltaTable.forPath(spark, path) mack.latest_version(delta_table) >> 2","title":"Get Latest Delta Table Version"},{"location":"#append-data-with-constraints","text":"The constraint_append function helps to append records to an existing Delta table even if there are records in the append dataframe that violate table constraints (both check and not null constraints), these records are appended to an existing quarantine Delta table instead of the target table. If the quarantine Delta table is set to None , those records that violate table constraints are simply thrown out. Suppose you have the following target Delta table with the following schema and constraints: schema: col1 int not null col2 string null col3 string null check constraints: col1_constraint: (col1 > 0) col2_constraint: (col2 != 'Z') +----+----+----+ |col1|col2|col3| +----+----+----+ | 1| A| B| | 2| C| D| | 3| E| F| +----+----+----+ Suppose you have a quarantine Delta table with the same schema but without the constraints. Here is data to be appended: +----+----+----+ |col1|col2|col3| +----+----+----+ | | H| H| # violates col1 not null constraint | 0| Z| Z| # violates both col1_constraint and col2_constraint | 4| A| B| | 5| C| D| | 6| E| F| | 9| G| G| | 11| Z| Z| # violates col2_constraint +----+----+----+ Run the constraint_append function: mack.constraint_append(delta_table, append_df, quarantine_table) Here's the ending result in delta_table: +----+----+----+ |col1|col2|col3| +----+----+----+ | 1| A| B| | 2| C| D| | 3| E| F| | 4| A| B| | 5| C| D| | 6| E| F| | 9| G| G| +----+----+----+ Here's the ending result in quarantine_table: +----+----+----+ |col1|col2|col3| +----+----+----+ | | H| H| | 0| Z| Z| | 11| Z| Z| +----+----+----+ Notice that the records that violated either of the constraints are appended to the quarantine table all other records are appended to the target table and the append has not failed. If a normal append operation was run, then it would have failed on the constraint violation. If quarantine_table is set to None , records that violated either of the constraints are simply thrown out.","title":"Append data with constraints"},{"location":"#rename-a-delta-table","text":"This function is designed to rename a Delta table. It can operate either within a Databricks environment or with a standalone Spark session.","title":"Rename a Delta Table"},{"location":"#parameters","text":"delta_table ( DeltaTable ): An object representing the Delta table to be renamed. new_table_name ( str ): The new name for the table. table_location ( str , optional): The file path where the table is stored. If not provided, the function attempts to deduce the location from the DeltaTable object. Defaults to None . databricks ( bool , optional): A flag indicating the function's operational environment. Set to True if running within Databricks, otherwise, False . Defaults to False . spark_session ( pyspark.sql.SparkSession , optional): The Spark session. This is required when databricks is set to True . Defaults to None .","title":"Parameters:"},{"location":"#returns","text":"None","title":"Returns:"},{"location":"#raises","text":"TypeError : If the provided delta_table is not a DeltaTable object, or if databricks is set to True and spark_session is None .","title":"Raises:"},{"location":"#example-usage","text":"rename_delta_table(existing_delta_table, \"new_table_name\")","title":"Example Usage:"},{"location":"#dictionary","text":"We're leveraging the following terminology defined here . Natural key: an attribute that can uniquely identify a row, and exists in the real world. Surrogate key: an attribute that can uniquely identify a row, and does not exist in the real world. Composite key: more than one attribute that when combined can uniquely identify a row. Primary key: the single unique identifier for the row. Candidate key: an attribute that could be the primary key. Alternate key: a candidate key that is not the primary key. Unique key: an attribute that can be unique on the table. Can also be called an alternate key. Foreign key: an attribute that is used to refer to another record in another table.","title":"Dictionary"},{"location":"#project-maintainers","text":"Matthew Powers aka MrPowers Robert Kossendey aka robertkossendey Souvik Pratiher aka souvik-databricks","title":"Project maintainers"},{"location":"#project-philosophy","text":"The mack library is designed to make common Delta Lake data tasks easier. You don't need to use mack of course. You can write the logic yourself. If you don't want to add a dependency to your project, you can also easily copy / paste the functions from mack. The functions in this library are intentionally designed to be easy to copy and paste. Let's look at some of the reasons you may want to add mack as a dependency.","title":"Project philosophy"},{"location":"#exposing-nice-public-interfaces","text":"The public interface (and only the public interface) is available via the mack namespace. When you run import mack , you can access the entirety of the public interface. No private implementation details are exposed in the mack namespace.","title":"Exposing nice public interfaces"},{"location":"#minimal-dependencies","text":"Mack only depends on Spark & Delta Lake. No other dependencies will be added to Mack. Spark users leverage a variety of runtimes and it's not always easy to add a dependency. You can run pip install mack and won't have to worry about resolving a lot of dependency conflicts. You can also Just attach a mack wheel file to a cluster to leverage the project.","title":"Minimal dependencies"},{"location":"#provide-best-practices-examples-for-the-community","text":"Mack strives to be a good example codebase for the PySpark / Delta Lake community. There aren't a lot of open source Delta Lake projects. There are even fewer that use good software engineering practices like CI and unit testing. You can use mack to help guide your design decisions in proprietary code repos.","title":"Provide best practices examples for the community"},{"location":"#stable-public-interfaces-and-long-term-support-after-10-release","text":"Mack reserves the right to make breaking public interface changes before the 1.0 release. We'll always minimize breaking changes whenever possible. After the 1.0 release, Mack will stricly follow Semantic Versioning 2.0 and will only make breaking public interface changes in major releases. Hopefully 1.0 will be the only major release and there won't have to be any breaking changes.","title":"Stable public interfaces and long term support after 1.0 release"},{"location":"#code-design","text":"Here are some of the code design principles used in Mack: We avoid classes whenever possible. Classes make it harder to copy / paste little chunks of code into notebooks. It's good to Stop Writing Classes . We try to make functions that are easy to copy. We do this by limiting functions that depend on other functions or classes. We'd rather nest a single use function in a public interface method than make it separate. Develop and then abstract. All code goes in a single file till the right abstractions become apparent. We'd rather have a large file than the wrong abstractions.","title":"Code design"},{"location":"#docker-environment","text":"The Dockerfile and docker-compose files provide a containerized way to run and develop with mack . The first time run docker build --tag=mack . to build the image. To execute the unit tests inside the Docker container, run docker-compose up test To drop into the running Docker container to develop, run docker run -it mack /bin/bash","title":"Docker Environment"},{"location":"#community","text":"","title":"Community"},{"location":"#blogs","text":"Daniel Beach (Confessions of a Data Guy): Simplify Delta Lake Complexity with mack. Bartosz Konieczny (waitingforcode): Simplified Delta Lake operations with Mack","title":"Blogs"},{"location":"#videos","text":"GeekCoders on YouTube: How I use MACK Library in Delta Lake using Databricks/PySpark","title":"Videos"},{"location":"reference/SUMMARY/","text":"mack","title":"API Docs"},{"location":"reference/mack/","text":"append_without_duplicates(delta_table, append_df, p_keys) Parameters: Name Type Description Default delta_table DeltaTable required append_df DataFrame required p_keys List [ str ] required Raises: Type Description TypeError Raises type error when input arguments have a invalid type. Source code in mack/__init__.py def append_without_duplicates( delta_table: DeltaTable, append_df: DataFrame, p_keys: List[str] ) -> None: \"\"\" <description> :param delta_table: <description> :type delta_table: DeltaTable :param append_df: <description> :type append_df: DataFrame :param p_keys: <description> :type p_keys: List[str] :raises TypeError: Raises type error when input arguments have a invalid type. \"\"\" if not isinstance(delta_table, DeltaTable): raise TypeError(\"An existing delta table must be specified.\") condition_columns = [] for column in p_keys: condition_columns.append(f\"old.{column} = new.{column}\") condition_columns = \" AND \".join(condition_columns) deduplicated_append_df = append_df.drop_duplicates(p_keys) # Insert records without duplicates delta_table.alias(\"old\").merge( deduplicated_append_df.alias(\"new\"), condition_columns ).whenNotMatchedInsertAll().execute() constraint_append(delta_table, append_df, quarantine_table) Parameters: Name Type Description Default delta_table DeltaTable required append_df DataFrame required quarantine_table DeltaTable required Raises: Type Description TypeError Raises type error when input arguments have an invalid type. TypeError Raises type error when delta_table has no constraints. Source code in mack/__init__.py def constraint_append( delta_table: DeltaTable, append_df: DataFrame, quarantine_table: DeltaTable ): \"\"\" <description> :param delta_table: <description> :type delta_table: DeltaTable :param append_df: <description> :type append_df: DataFrame :param quarantine_table: <description> :type quarantine_table: DeltaTable :raises TypeError: Raises type error when input arguments have an invalid type. :raises TypeError: Raises type error when delta_table has no constraints. \"\"\" if not isinstance(delta_table, DeltaTable): raise TypeError(\"An existing delta table must be specified for delta_table.\") if not isinstance(append_df, DataFrame): raise TypeError(\"You must provide a DataFrame that is to be appended.\") if quarantine_table is not None and not isinstance(quarantine_table, DeltaTable): raise TypeError( \"An existing delta table must be specified for quarantine_table.\" ) properties = delta_table.detail().select(\"properties\").collect()[0][\"properties\"] check_constraints = [ v for k, v in properties.items() if k.startswith(\"delta.constraints\") ] # add null checks fields = delta_table.toDF().schema.fields null_constraints = [ f\"{field.name} is not null\" for field in fields if not field.nullable ] constraints = check_constraints + null_constraints if not constraints: raise TypeError(\"There are no constraints present in the target delta table\") target_details = delta_table.detail().select(\"location\").collect()[0] if quarantine_table: quarantine_details = quarantine_table.detail().select(\"location\").collect()[0] quarantine_df = append_df.filter( \"not (\" + \" and \".join([c for c in constraints]) + \")\" ) ( quarantine_df.write.format(\"delta\") .mode(\"append\") .option(\"mergeSchema\", \"true\") .save(quarantine_details[\"location\"]) ) filtered_df = append_df.filter(\" and \".join([c for c in constraints])) ( filtered_df.write.format(\"delta\") .mode(\"append\") .option(\"mergeSchema\", \"true\") .save(target_details[\"location\"]) ) copy_table(delta_table, target_path='', target_table='') Parameters: Name Type Description Default delta_table DeltaTable required target_path str , defaults to empty string. '' target_table str , defaults to empty string. '' Raises: Type Description TypeError Raises type error when input arguments have a invalid type, are missing or are empty. Source code in mack/__init__.py def copy_table( delta_table: DeltaTable, target_path: str = \"\", target_table: str = \"\" ) -> None: \"\"\" <description> :param delta_table: <description> :type delta_table: DeltaTable :param target_path: <description>, defaults to empty string. :type target_path: str :param target_table: <description>, defaults to empty string. :type target_table: str :raises TypeError: Raises type error when input arguments have a invalid type, are missing or are empty. \"\"\" if not isinstance(delta_table, DeltaTable): raise TypeError(\"An existing delta table must be specified.\") if not target_path and not target_table: raise TypeError(\"Either target_path or target_table must be specified.\") origin_table = delta_table.toDF() details = delta_table.detail().select(\"partitionColumns\", \"properties\").collect()[0] if target_table: ( origin_table.write.format(\"delta\") .partitionBy(details[\"partitionColumns\"]) .options(**details[\"properties\"]) .saveAsTable(target_table) ) else: ( origin_table.write.format(\"delta\") .partitionBy(details[\"partitionColumns\"]) .options(**details[\"properties\"]) .save(target_path) ) delta_file_sizes(delta_table) Parameters: Name Type Description Default delta_table DeltaTable required Returns: Type Description Dict[str, int] Source code in mack/__init__.py def delta_file_sizes(delta_table: DeltaTable) -> Dict[str, int]: \"\"\" <description> :param delta_table: <description> :type delta_table: DeltaTable :returns: <description> :rtype: Dict[str, int] \"\"\" details = delta_table.detail().select(\"numFiles\", \"sizeInBytes\").collect()[0] size_in_bytes, number_of_files = details[\"sizeInBytes\"], details[\"numFiles\"] average_file_size_in_bytes = round(size_in_bytes / number_of_files, 0) return { \"size_in_bytes\": size_in_bytes, \"number_of_files\": number_of_files, \"average_file_size_in_bytes\": average_file_size_in_bytes, } drop_duplicates(delta_table, duplication_columns) Parameters: Name Type Description Default delta_table DeltaTable required duplication_columns List [ str ] required Raises: Type Description TypeError Raises type error when input arguments have a invalid type, are missing or are empty. Source code in mack/__init__.py def drop_duplicates(delta_table: DeltaTable, duplication_columns: List[str]) -> None: \"\"\" <description> :param delta_table: <description> :type delta_table: DeltaTable :param duplication_columns: <description> :type duplication_columns: List[str] :raises TypeError: Raises type error when input arguments have a invalid type, are missing or are empty. \"\"\" if not isinstance(delta_table, DeltaTable): raise TypeError(\"An existing delta table must be specified.\") if not duplication_columns or len(duplication_columns) == 0: raise TypeError(\"A duplication column must be specified.\") data_frame = delta_table.toDF() details = delta_table.detail().select(\"location\").collect()[0] ( data_frame.drop_duplicates(duplication_columns) .write.format(\"delta\") .mode(\"overwrite\") .save(details[\"location\"]) ) drop_duplicates_pkey(delta_table, primary_key, duplication_columns) Parameters: Name Type Description Default delta_table DeltaTable required primary_key str required duplication_columns List [ str ] required Raises: Type Description TypeError Raises type error when input arguments have a invalid type, are missing or are empty. TypeError Raises type error when required columns are missing in the provided delta table. Source code in mack/__init__.py def drop_duplicates_pkey( delta_table: DeltaTable, primary_key: str, duplication_columns: List[str] ) -> None: \"\"\" <description> :param delta_table: <description> :type delta_table: DeltaTable :param primary_key: <description> :type primary_key: str :param duplication_columns: <description> :type duplication_columns: List[str] :raises TypeError: Raises type error when input arguments have a invalid type, are missing or are empty. :raises TypeError: Raises type error when required columns are missing in the provided delta table. \"\"\" if not isinstance(delta_table, DeltaTable): raise TypeError(\"An existing delta table must be specified.\") if not primary_key: raise TypeError(\"A unique primary key must be specified.\") if not duplication_columns or len(duplication_columns) == 0: raise TypeError(\"A duplication column must be specified.\") if primary_key in duplication_columns: raise TypeError(\"Primary key must not be part of the duplication columns.\") data_frame = delta_table.toDF() # Make sure that all the required columns are present in the provided delta table append_data_columns = data_frame.columns required_columns = [primary_key] + duplication_columns for required_column in required_columns: if required_column not in append_data_columns: raise TypeError( f\"The base table has these columns {append_data_columns!r}, but these columns are required {required_columns!r}\" ) q = [] duplicate_records = ( data_frame.withColumn( \"row_number\", row_number().over( Window().partitionBy(duplication_columns).orderBy(primary_key) ), ) .filter(col(\"row_number\") > 1) .drop(\"row_number\") .distinct() ) for column in required_columns: q.append(f\"old.{column} = new.{column}\") q = \" AND \".join(q) # Remove all the duplicate records delta_table.alias(\"old\").merge( duplicate_records.alias(\"new\"), q ).whenMatchedDelete().execute() find_composite_key_candidates(df, exclude_cols=None) Parameters: Name Type Description Default df Union [ DeltaTable , DataFrame ] required exclude_cols List [ str ] None Returns: Type Description List Raises: Type Description TypeError Raises type error when no composite key can be found. Source code in mack/__init__.py def find_composite_key_candidates( df: Union[DeltaTable, DataFrame], exclude_cols: List[str] = None ) -> List: \"\"\" <description> :param df: <description> :type df: DeltaTable or DataFrame :param exclude_cols: <description> :type exclude_cols: List[str], defaults to None. :raises TypeError: Raises type error when no composite key can be found. :returns: <description> :rtype: List \"\"\" if type(df) == DeltaTable: df = df.toDF() if exclude_cols is None: exclude_cols = [] df_col_excluded = df.drop(*exclude_cols) total_cols = len(df_col_excluded.columns) total_row_count = df_col_excluded.distinct().count() for n in range(1, len(df_col_excluded.columns) + 1): for c in combinations(df_col_excluded.columns, n): if df_col_excluded.select(*c).distinct().count() == total_row_count: if len(df_col_excluded.select(*c).columns) == total_cols: raise ValueError(\"No composite key candidates could be identified.\") return list(df_col_excluded.select(*c).columns) humanize_bytes(n) Parameters: Name Type Description Default n int required Returns: Type Description str Source code in mack/__init__.py def humanize_bytes(n: int) -> str: \"\"\" <description> :param n: <description> :type n: int :returns: <description> :rtype: str \"\"\" kilobyte = 1000 for prefix, k in ( (\"PB\", kilobyte**5), (\"TB\", kilobyte**4), (\"GB\", kilobyte**3), (\"MB\", kilobyte**2), (\"kB\", kilobyte**1), ): if n >= k * 0.9: return f\"{n / k:.2f} {prefix}\" return f\"{n} B\" humanize_bytes_binary(n) Parameters: Name Type Description Default n int required Returns: Type Description str Source code in mack/__init__.py def humanize_bytes_binary(n: int) -> str: \"\"\" <description> :param n: <description> :type n: int :returns: <description> :rtype: str \"\"\" kibibyte = 1024 for prefix, k in ( (\"PB\", kibibyte**5), (\"TB\", kibibyte**4), (\"GB\", kibibyte**3), (\"MB\", kibibyte**2), (\"kB\", kibibyte**1), ): if n >= k * 0.9: return f\"{n / k:.2f} {prefix}\" return f\"{n} B\" is_composite_key_candidate(delta_table, cols) Parameters: Name Type Description Default delta_table DeltaTable required cols List [ str ] required Returns: Type Description bool Raises: Type Description TypeError Raises type error when input arguments have a invalid type or are missing. TypeError Raises type error when required columns are not in dataframe columns. Source code in mack/__init__.py def is_composite_key_candidate(delta_table: DeltaTable, cols: List[str]) -> bool: \"\"\" <description> :param delta_table: <description> :type delta_table: DeltaTable :param cols: <description> :type cols: List[str] :raises TypeError: Raises type error when input arguments have a invalid type or are missing. :raises TypeError: Raises type error when required columns are not in dataframe columns. :returns: <description> :rtype: bool \"\"\" if not isinstance(delta_table, DeltaTable): raise TypeError(\"An existing delta table must be specified.\") if not cols or len(cols) == 0: raise TypeError(\"At least one column must be specified.\") data_frame = delta_table.toDF() for required_column in cols: if required_column not in data_frame.columns: raise TypeError( f\"The base table has these columns {data_frame.columns!r}, but these columns are required {cols!r}\" ) duplicate_records = ( data_frame.withColumn( \"amount_of_records\", count(\"*\").over(Window.partitionBy(cols)), ) .filter(col(\"amount_of_records\") > 1) .drop(\"amount_of_records\") ) if len(duplicate_records.take(1)) == 0: return True return False kill_duplicates(delta_table, duplication_columns) Parameters: Name Type Description Default delta_table DeltaTable required duplication_columns List [ str ] required Raises: Type Description TypeError Raises type error when input arguments have a invalid type or are empty. TypeError Raises type error when required columns are missing in the provided delta table. Source code in mack/__init__.py def kill_duplicates(delta_table: DeltaTable, duplication_columns: List[str]) -> None: \"\"\" <description> :param delta_table: <description> :type delta_table: DeltaTable :param duplication_columns: <description> :type duplication_columns: List[str] :raises TypeError: Raises type error when input arguments have a invalid type or are empty. :raises TypeError: Raises type error when required columns are missing in the provided delta table. \"\"\" if not isinstance(delta_table, DeltaTable): raise TypeError(\"An existing delta table must be specified.\") if not duplication_columns or len(duplication_columns) == 0: raise TypeError(\"Duplication columns must be specified\") data_frame = delta_table.toDF() # Make sure that all the required columns are present in the provided delta table append_data_columns = data_frame.columns for required_column in duplication_columns: if required_column not in append_data_columns: raise TypeError( f\"The base table has these columns {append_data_columns!r}, but these columns are required {duplication_columns!r}\" ) q = [] duplicate_records = ( data_frame.withColumn( \"amount_of_records\", count(\"*\").over(Window.partitionBy(duplication_columns)), ) .filter(col(\"amount_of_records\") > 1) .drop(\"amount_of_records\") .distinct() ) for column in duplication_columns: q.append(f\"old.{column} = new.{column}\") q = \" AND \".join(q) # Remove all the duplicate records delta_table.alias(\"old\").merge( duplicate_records.alias(\"new\"), q ).whenMatchedDelete().execute() latest_version(delta_table) Parameters: Name Type Description Default delta_table DeltaTable required Returns: Type Description float Source code in mack/__init__.py def latest_version(delta_table: DeltaTable) -> float: \"\"\" <description> :param delta_table: <description> :type delta_table: DeltaTable :returns: <description> :rtype: float \"\"\" version = delta_table.history().agg(max(\"version\")).collect()[0][0] return version rename_delta_table(delta_table, new_table_name, table_location=None, databricks=False, spark_session=None) Renames a Delta table to a new name. This function can be used in a Databricks environment or with a standalone Spark session. Parameters: delta_table (DeltaTable): The DeltaTable object representing the table to be renamed. new_table_name (str): The new name for the table. table_location (str, optional): The file path where the table is stored. Defaults to None. If None, the function will attempt to determine the location from the DeltaTable object. databricks (bool, optional): A flag indicating whether the function is being run in a Databricks environment. Defaults to False. If True, a SparkSession must be provided. spark_session (pyspark.sql.SparkSession, optional): The Spark session. Defaults to None. Required if databricks is set to True. Returns: None Raises: TypeError: If the provided delta_table is not a DeltaTable object, or if databricks is True and spark_session is None. Example Usage: rename_delta_table(existing_delta_table, \"new_table_name\") Source code in mack/__init__.py def rename_delta_table( delta_table: DeltaTable, new_table_name: str, table_location: str = None, databricks: bool = False, spark_session: pyspark.sql.SparkSession = None, ) -> None: \"\"\" Renames a Delta table to a new name. This function can be used in a Databricks environment or with a standalone Spark session. Parameters: delta_table (DeltaTable): The DeltaTable object representing the table to be renamed. new_table_name (str): The new name for the table. table_location (str, optional): The file path where the table is stored. Defaults to None. If None, the function will attempt to determine the location from the DeltaTable object. databricks (bool, optional): A flag indicating whether the function is being run in a Databricks environment. Defaults to False. If True, a SparkSession must be provided. spark_session (pyspark.sql.SparkSession, optional): The Spark session. Defaults to None. Required if `databricks` is set to True. Returns: None Raises: TypeError: If the provided `delta_table` is not a DeltaTable object, or if `databricks` is True and `spark_session` is None. Example Usage: >>> rename_delta_table(existing_delta_table, \"new_table_name\") \"\"\" if not isinstance(delta_table, DeltaTable): raise TypeError(\"An existing delta table must be specified for delta_table.\") if databricks and spark_session is None: raise TypeError(\"A spark session must be specified for databricks.\") if databricks: spark_session.sql(f\"ALTER TABLE {delta_table.name} RENAME TO {new_table_name}\") else: delta_table.toDF().write.format(\"delta\").mode(\"overwrite\").saveAsTable( new_table_name ) show_delta_file_sizes(delta_table, humanize_binary=False) Parameters: Name Type Description Default delta_table DeltaTable required humanize_binary bool False Returns: Type Description None Source code in mack/__init__.py def show_delta_file_sizes( delta_table: DeltaTable, humanize_binary: bool = False ) -> None: \"\"\" <description> :param delta_table: <description> :type delta_table: DeltaTable :param humanize_binary: <description> :type humanize_binary: bool :returns: <description> :rtype: None \"\"\" details = delta_table.detail().select(\"numFiles\", \"sizeInBytes\").collect()[0] size_in_bytes, number_of_files = details[\"sizeInBytes\"], details[\"numFiles\"] average_file_size_in_bytes = round(size_in_bytes / number_of_files, 0) if humanize_binary: humanized_size_in_bytes = humanize_bytes_binary(size_in_bytes) humanized_average_file_size = humanize_bytes_binary(average_file_size_in_bytes) else: humanized_size_in_bytes = humanize_bytes(size_in_bytes) humanized_average_file_size = humanize_bytes(average_file_size_in_bytes) humanized_number_of_files = f\"{number_of_files:,}\" print( f\"The delta table contains {humanized_number_of_files} files with a size of {humanized_size_in_bytes}.\" + f\" The average file size is {humanized_average_file_size}\" ) type_2_scd_generic_upsert(delta_table, updates_df, primary_key, attr_col_names, is_current_col_name, effective_time_col_name, end_time_col_name) Parameters: Name Type Description Default delta_table DeltaTable DeltaTable required updates_df DataFrame required primary_key str required attr_col_names List [ str ] required is_current_col_name str required effective_time_col_name str required end_time_col_name str required Returns: Type Description None Raises: Type Description TypeError Raises type error when required column names are not in the base table. TypeError Raises type error when required column names for updates are not in the attributes columns list. Source code in mack/__init__.py def type_2_scd_generic_upsert( delta_table: DeltaTable, updates_df: DataFrame, primary_key: str, attr_col_names: List[str], is_current_col_name: str, effective_time_col_name: str, end_time_col_name: str, ) -> None: \"\"\" <description> :param delta_table: DeltaTable :type path: str :param updates_df: <description> :type updates_df: DataFrame :param primary_key: <description> :type primary_key: str :param attr_col_names: <description> :type attr_col_names: List[str] :param is_current_col_name: <description> :type is_current_col_name: str :param effective_time_col_name: <description> :type effective_time_col_name: str :param end_time_col_name: <description> :type effective_time_col_name: str :raises TypeError: Raises type error when required column names are not in the base table. :raises TypeError: Raises type error when required column names for updates are not in the attributes columns list. :returns: <description> :rtype: None \"\"\" # validate the existing Delta table base_col_names = delta_table.toDF().columns required_base_col_names = ( [primary_key] + attr_col_names + [is_current_col_name, effective_time_col_name, end_time_col_name] ) if sorted(base_col_names) != sorted(required_base_col_names): raise TypeError( f\"The base table has these columns {base_col_names!r}, but these columns are required {required_base_col_names!r}\" ) # validate the updates DataFrame updates_col_names = updates_df.columns required_updates_col_names = ( [primary_key] + attr_col_names + [effective_time_col_name] ) if sorted(updates_col_names) != sorted(required_updates_col_names): raise TypeError( f\"The updates DataFrame has these columns {updates_col_names!r}, but these columns are required {required_updates_col_names!r}\" ) # perform the upsert updates_attrs = list( map(lambda attr: f\"updates.{attr} <> base.{attr}\", attr_col_names) ) updates_attrs = \" OR \".join(updates_attrs) staged_updates_attrs = list( map(lambda attr: f\"staged_updates.{attr} <> base.{attr}\", attr_col_names) ) staged_updates_attrs = \" OR \".join(staged_updates_attrs) staged_part_1 = ( updates_df.alias(\"updates\") .join(delta_table.toDF().alias(\"base\"), primary_key) .where(f\"base.{is_current_col_name} = true AND ({updates_attrs})\") .selectExpr(\"NULL as mergeKey\", \"updates.*\") ) staged_part_2 = updates_df.selectExpr(f\"{primary_key} as mergeKey\", \"*\") staged_updates = staged_part_1.union(staged_part_2) thing = {} for attr in attr_col_names: thing[attr] = f\"staged_updates.{attr}\" thing2 = { primary_key: f\"staged_updates.{primary_key}\", is_current_col_name: \"true\", effective_time_col_name: f\"staged_updates.{effective_time_col_name}\", end_time_col_name: \"null\", } res_thing = {**thing, **thing2} res = ( delta_table.alias(\"base\") .merge( source=staged_updates.alias(\"staged_updates\"), condition=pyspark.sql.functions.expr(f\"base.{primary_key} = mergeKey\"), ) .whenMatchedUpdate( condition=f\"base.{is_current_col_name} = true AND ({staged_updates_attrs})\", set={ is_current_col_name: \"false\", end_time_col_name: f\"staged_updates.{effective_time_col_name}\", }, ) .whenNotMatchedInsert(values=res_thing) .execute() ) return res type_2_scd_upsert(delta_table, updates_df, primary_key, attr_col_names) Parameters: Name Type Description Default path DeltaTable required updates_df DataFrame required primary_key str required attr_col_names List [ str ] required Returns: Type Description None Source code in mack/__init__.py def type_2_scd_upsert( delta_table: DeltaTable, updates_df: DataFrame, primary_key: str, attr_col_names: List[str], ) -> None: \"\"\" <description> :param path: <description> :type path: DeltaTable :param updates_df: <description> :type updates_df: DataFrame :param primary_key: <description> :type primary_key: str :param attr_col_names: <description> :type attr_col_names: List[str] :returns: <description> :rtype: None \"\"\" return type_2_scd_generic_upsert( delta_table, updates_df, primary_key, attr_col_names, \"is_current\", \"effective_time\", \"end_time\", ) validate_append(delta_table, append_df, required_cols, optional_cols) Parameters: Name Type Description Default delta_table DeltaTable required append_df DataFrame required required_cols List [ str ] required optional_cols List [ str ] required Raises: Type Description TypeError Raises type error when input arguments have a invalid type, are missing or are empty. TypeError Raises type error when required columns are missing in the provided delta table. TypeError Raises type error when column in append dataframe is not part of the original delta table.. Source code in mack/__init__.py def validate_append( delta_table: DeltaTable, append_df: DataFrame, required_cols: List[str], optional_cols: List[str], ) -> None: \"\"\" <description> :param delta_table: <description> :type delta_table: DeltaTable :param append_df: <description> :type append_df: DataFrame :param required_cols: <description> :type required_cols: List[str] :param optional_cols: <description> :type optional_cols: List[str] :raises TypeError: Raises type error when input arguments have a invalid type, are missing or are empty. :raises TypeError: Raises type error when required columns are missing in the provided delta table. :raises TypeError: Raises type error when column in append dataframe is not part of the original delta table.. \"\"\" if not isinstance(delta_table, DeltaTable): raise TypeError(\"An existing delta table must be specified.\") if not isinstance(append_df, DataFrame): raise TypeError(\"You must provide a DataFrame that is to be appended.\") append_data_columns = append_df.columns for required_column in required_cols: if required_column not in append_data_columns: raise TypeError( f\"The base Delta table has these columns {append_data_columns!r}, but these columns are required {required_cols!r}\" ) table_columns = delta_table.toDF().columns for column in append_data_columns: if column not in table_columns and column not in optional_cols: raise TypeError( f\"The column {column!r} is not part of the current Delta table.\" + \" If you want to add the column to the table you must set the optional_cols parameter.\" ) details = delta_table.detail().select(\"location\").collect()[0] ( append_df.write.format(\"delta\") .mode(\"append\") .option(\"mergeSchema\", \"true\") .save(details[\"location\"]) ) with_md5_cols(df, cols, output_col_name=None) Parameters: Name Type Description Default df Union [ DeltaTable , DataFrame ] required cols List [ str ] required output_col_name Optional [ str ] None Returns: Type Description DataFrame Raises: Type Description TypeError Raises type error when no composite key can be found. Source code in mack/__init__.py def with_md5_cols( df: Union[DeltaTable, DataFrame], cols: List[str], output_col_name: Optional[str] = None, ) -> DataFrame: \"\"\" <description> :param df: <description> :type df: DeltaTable or DataFrame :param cols: <description> :type cols: List[str] :param output_col_name: <description> :type output_col_name: str, defaults to empty string. :raises TypeError: Raises type error when no composite key can be found. :returns: <description> :rtype: DataFrame \"\"\" if output_col_name is None: output_col_name = \"_\".join([\"md5\"] + cols) if type(df) == DeltaTable: df = df.toDF() return df.withColumn(output_col_name, md5(concat_ws(\"||\", *cols)))","title":"Index"},{"location":"reference/mack/#mack.append_without_duplicates","text":"Parameters: Name Type Description Default delta_table DeltaTable required append_df DataFrame required p_keys List [ str ] required Raises: Type Description TypeError Raises type error when input arguments have a invalid type. Source code in mack/__init__.py def append_without_duplicates( delta_table: DeltaTable, append_df: DataFrame, p_keys: List[str] ) -> None: \"\"\" <description> :param delta_table: <description> :type delta_table: DeltaTable :param append_df: <description> :type append_df: DataFrame :param p_keys: <description> :type p_keys: List[str] :raises TypeError: Raises type error when input arguments have a invalid type. \"\"\" if not isinstance(delta_table, DeltaTable): raise TypeError(\"An existing delta table must be specified.\") condition_columns = [] for column in p_keys: condition_columns.append(f\"old.{column} = new.{column}\") condition_columns = \" AND \".join(condition_columns) deduplicated_append_df = append_df.drop_duplicates(p_keys) # Insert records without duplicates delta_table.alias(\"old\").merge( deduplicated_append_df.alias(\"new\"), condition_columns ).whenNotMatchedInsertAll().execute()","title":"append_without_duplicates()"},{"location":"reference/mack/#mack.constraint_append","text":"Parameters: Name Type Description Default delta_table DeltaTable required append_df DataFrame required quarantine_table DeltaTable required Raises: Type Description TypeError Raises type error when input arguments have an invalid type. TypeError Raises type error when delta_table has no constraints. Source code in mack/__init__.py def constraint_append( delta_table: DeltaTable, append_df: DataFrame, quarantine_table: DeltaTable ): \"\"\" <description> :param delta_table: <description> :type delta_table: DeltaTable :param append_df: <description> :type append_df: DataFrame :param quarantine_table: <description> :type quarantine_table: DeltaTable :raises TypeError: Raises type error when input arguments have an invalid type. :raises TypeError: Raises type error when delta_table has no constraints. \"\"\" if not isinstance(delta_table, DeltaTable): raise TypeError(\"An existing delta table must be specified for delta_table.\") if not isinstance(append_df, DataFrame): raise TypeError(\"You must provide a DataFrame that is to be appended.\") if quarantine_table is not None and not isinstance(quarantine_table, DeltaTable): raise TypeError( \"An existing delta table must be specified for quarantine_table.\" ) properties = delta_table.detail().select(\"properties\").collect()[0][\"properties\"] check_constraints = [ v for k, v in properties.items() if k.startswith(\"delta.constraints\") ] # add null checks fields = delta_table.toDF().schema.fields null_constraints = [ f\"{field.name} is not null\" for field in fields if not field.nullable ] constraints = check_constraints + null_constraints if not constraints: raise TypeError(\"There are no constraints present in the target delta table\") target_details = delta_table.detail().select(\"location\").collect()[0] if quarantine_table: quarantine_details = quarantine_table.detail().select(\"location\").collect()[0] quarantine_df = append_df.filter( \"not (\" + \" and \".join([c for c in constraints]) + \")\" ) ( quarantine_df.write.format(\"delta\") .mode(\"append\") .option(\"mergeSchema\", \"true\") .save(quarantine_details[\"location\"]) ) filtered_df = append_df.filter(\" and \".join([c for c in constraints])) ( filtered_df.write.format(\"delta\") .mode(\"append\") .option(\"mergeSchema\", \"true\") .save(target_details[\"location\"]) )","title":"constraint_append()"},{"location":"reference/mack/#mack.copy_table","text":"Parameters: Name Type Description Default delta_table DeltaTable required target_path str , defaults to empty string. '' target_table str , defaults to empty string. '' Raises: Type Description TypeError Raises type error when input arguments have a invalid type, are missing or are empty. Source code in mack/__init__.py def copy_table( delta_table: DeltaTable, target_path: str = \"\", target_table: str = \"\" ) -> None: \"\"\" <description> :param delta_table: <description> :type delta_table: DeltaTable :param target_path: <description>, defaults to empty string. :type target_path: str :param target_table: <description>, defaults to empty string. :type target_table: str :raises TypeError: Raises type error when input arguments have a invalid type, are missing or are empty. \"\"\" if not isinstance(delta_table, DeltaTable): raise TypeError(\"An existing delta table must be specified.\") if not target_path and not target_table: raise TypeError(\"Either target_path or target_table must be specified.\") origin_table = delta_table.toDF() details = delta_table.detail().select(\"partitionColumns\", \"properties\").collect()[0] if target_table: ( origin_table.write.format(\"delta\") .partitionBy(details[\"partitionColumns\"]) .options(**details[\"properties\"]) .saveAsTable(target_table) ) else: ( origin_table.write.format(\"delta\") .partitionBy(details[\"partitionColumns\"]) .options(**details[\"properties\"]) .save(target_path) )","title":"copy_table()"},{"location":"reference/mack/#mack.delta_file_sizes","text":"Parameters: Name Type Description Default delta_table DeltaTable required Returns: Type Description Dict[str, int] Source code in mack/__init__.py def delta_file_sizes(delta_table: DeltaTable) -> Dict[str, int]: \"\"\" <description> :param delta_table: <description> :type delta_table: DeltaTable :returns: <description> :rtype: Dict[str, int] \"\"\" details = delta_table.detail().select(\"numFiles\", \"sizeInBytes\").collect()[0] size_in_bytes, number_of_files = details[\"sizeInBytes\"], details[\"numFiles\"] average_file_size_in_bytes = round(size_in_bytes / number_of_files, 0) return { \"size_in_bytes\": size_in_bytes, \"number_of_files\": number_of_files, \"average_file_size_in_bytes\": average_file_size_in_bytes, }","title":"delta_file_sizes()"},{"location":"reference/mack/#mack.drop_duplicates","text":"Parameters: Name Type Description Default delta_table DeltaTable required duplication_columns List [ str ] required Raises: Type Description TypeError Raises type error when input arguments have a invalid type, are missing or are empty. Source code in mack/__init__.py def drop_duplicates(delta_table: DeltaTable, duplication_columns: List[str]) -> None: \"\"\" <description> :param delta_table: <description> :type delta_table: DeltaTable :param duplication_columns: <description> :type duplication_columns: List[str] :raises TypeError: Raises type error when input arguments have a invalid type, are missing or are empty. \"\"\" if not isinstance(delta_table, DeltaTable): raise TypeError(\"An existing delta table must be specified.\") if not duplication_columns or len(duplication_columns) == 0: raise TypeError(\"A duplication column must be specified.\") data_frame = delta_table.toDF() details = delta_table.detail().select(\"location\").collect()[0] ( data_frame.drop_duplicates(duplication_columns) .write.format(\"delta\") .mode(\"overwrite\") .save(details[\"location\"]) )","title":"drop_duplicates()"},{"location":"reference/mack/#mack.drop_duplicates_pkey","text":"Parameters: Name Type Description Default delta_table DeltaTable required primary_key str required duplication_columns List [ str ] required Raises: Type Description TypeError Raises type error when input arguments have a invalid type, are missing or are empty. TypeError Raises type error when required columns are missing in the provided delta table. Source code in mack/__init__.py def drop_duplicates_pkey( delta_table: DeltaTable, primary_key: str, duplication_columns: List[str] ) -> None: \"\"\" <description> :param delta_table: <description> :type delta_table: DeltaTable :param primary_key: <description> :type primary_key: str :param duplication_columns: <description> :type duplication_columns: List[str] :raises TypeError: Raises type error when input arguments have a invalid type, are missing or are empty. :raises TypeError: Raises type error when required columns are missing in the provided delta table. \"\"\" if not isinstance(delta_table, DeltaTable): raise TypeError(\"An existing delta table must be specified.\") if not primary_key: raise TypeError(\"A unique primary key must be specified.\") if not duplication_columns or len(duplication_columns) == 0: raise TypeError(\"A duplication column must be specified.\") if primary_key in duplication_columns: raise TypeError(\"Primary key must not be part of the duplication columns.\") data_frame = delta_table.toDF() # Make sure that all the required columns are present in the provided delta table append_data_columns = data_frame.columns required_columns = [primary_key] + duplication_columns for required_column in required_columns: if required_column not in append_data_columns: raise TypeError( f\"The base table has these columns {append_data_columns!r}, but these columns are required {required_columns!r}\" ) q = [] duplicate_records = ( data_frame.withColumn( \"row_number\", row_number().over( Window().partitionBy(duplication_columns).orderBy(primary_key) ), ) .filter(col(\"row_number\") > 1) .drop(\"row_number\") .distinct() ) for column in required_columns: q.append(f\"old.{column} = new.{column}\") q = \" AND \".join(q) # Remove all the duplicate records delta_table.alias(\"old\").merge( duplicate_records.alias(\"new\"), q ).whenMatchedDelete().execute()","title":"drop_duplicates_pkey()"},{"location":"reference/mack/#mack.find_composite_key_candidates","text":"Parameters: Name Type Description Default df Union [ DeltaTable , DataFrame ] required exclude_cols List [ str ] None Returns: Type Description List Raises: Type Description TypeError Raises type error when no composite key can be found. Source code in mack/__init__.py def find_composite_key_candidates( df: Union[DeltaTable, DataFrame], exclude_cols: List[str] = None ) -> List: \"\"\" <description> :param df: <description> :type df: DeltaTable or DataFrame :param exclude_cols: <description> :type exclude_cols: List[str], defaults to None. :raises TypeError: Raises type error when no composite key can be found. :returns: <description> :rtype: List \"\"\" if type(df) == DeltaTable: df = df.toDF() if exclude_cols is None: exclude_cols = [] df_col_excluded = df.drop(*exclude_cols) total_cols = len(df_col_excluded.columns) total_row_count = df_col_excluded.distinct().count() for n in range(1, len(df_col_excluded.columns) + 1): for c in combinations(df_col_excluded.columns, n): if df_col_excluded.select(*c).distinct().count() == total_row_count: if len(df_col_excluded.select(*c).columns) == total_cols: raise ValueError(\"No composite key candidates could be identified.\") return list(df_col_excluded.select(*c).columns)","title":"find_composite_key_candidates()"},{"location":"reference/mack/#mack.humanize_bytes","text":"Parameters: Name Type Description Default n int required Returns: Type Description str Source code in mack/__init__.py def humanize_bytes(n: int) -> str: \"\"\" <description> :param n: <description> :type n: int :returns: <description> :rtype: str \"\"\" kilobyte = 1000 for prefix, k in ( (\"PB\", kilobyte**5), (\"TB\", kilobyte**4), (\"GB\", kilobyte**3), (\"MB\", kilobyte**2), (\"kB\", kilobyte**1), ): if n >= k * 0.9: return f\"{n / k:.2f} {prefix}\" return f\"{n} B\"","title":"humanize_bytes()"},{"location":"reference/mack/#mack.humanize_bytes_binary","text":"Parameters: Name Type Description Default n int required Returns: Type Description str Source code in mack/__init__.py def humanize_bytes_binary(n: int) -> str: \"\"\" <description> :param n: <description> :type n: int :returns: <description> :rtype: str \"\"\" kibibyte = 1024 for prefix, k in ( (\"PB\", kibibyte**5), (\"TB\", kibibyte**4), (\"GB\", kibibyte**3), (\"MB\", kibibyte**2), (\"kB\", kibibyte**1), ): if n >= k * 0.9: return f\"{n / k:.2f} {prefix}\" return f\"{n} B\"","title":"humanize_bytes_binary()"},{"location":"reference/mack/#mack.is_composite_key_candidate","text":"Parameters: Name Type Description Default delta_table DeltaTable required cols List [ str ] required Returns: Type Description bool Raises: Type Description TypeError Raises type error when input arguments have a invalid type or are missing. TypeError Raises type error when required columns are not in dataframe columns. Source code in mack/__init__.py def is_composite_key_candidate(delta_table: DeltaTable, cols: List[str]) -> bool: \"\"\" <description> :param delta_table: <description> :type delta_table: DeltaTable :param cols: <description> :type cols: List[str] :raises TypeError: Raises type error when input arguments have a invalid type or are missing. :raises TypeError: Raises type error when required columns are not in dataframe columns. :returns: <description> :rtype: bool \"\"\" if not isinstance(delta_table, DeltaTable): raise TypeError(\"An existing delta table must be specified.\") if not cols or len(cols) == 0: raise TypeError(\"At least one column must be specified.\") data_frame = delta_table.toDF() for required_column in cols: if required_column not in data_frame.columns: raise TypeError( f\"The base table has these columns {data_frame.columns!r}, but these columns are required {cols!r}\" ) duplicate_records = ( data_frame.withColumn( \"amount_of_records\", count(\"*\").over(Window.partitionBy(cols)), ) .filter(col(\"amount_of_records\") > 1) .drop(\"amount_of_records\") ) if len(duplicate_records.take(1)) == 0: return True return False","title":"is_composite_key_candidate()"},{"location":"reference/mack/#mack.kill_duplicates","text":"Parameters: Name Type Description Default delta_table DeltaTable required duplication_columns List [ str ] required Raises: Type Description TypeError Raises type error when input arguments have a invalid type or are empty. TypeError Raises type error when required columns are missing in the provided delta table. Source code in mack/__init__.py def kill_duplicates(delta_table: DeltaTable, duplication_columns: List[str]) -> None: \"\"\" <description> :param delta_table: <description> :type delta_table: DeltaTable :param duplication_columns: <description> :type duplication_columns: List[str] :raises TypeError: Raises type error when input arguments have a invalid type or are empty. :raises TypeError: Raises type error when required columns are missing in the provided delta table. \"\"\" if not isinstance(delta_table, DeltaTable): raise TypeError(\"An existing delta table must be specified.\") if not duplication_columns or len(duplication_columns) == 0: raise TypeError(\"Duplication columns must be specified\") data_frame = delta_table.toDF() # Make sure that all the required columns are present in the provided delta table append_data_columns = data_frame.columns for required_column in duplication_columns: if required_column not in append_data_columns: raise TypeError( f\"The base table has these columns {append_data_columns!r}, but these columns are required {duplication_columns!r}\" ) q = [] duplicate_records = ( data_frame.withColumn( \"amount_of_records\", count(\"*\").over(Window.partitionBy(duplication_columns)), ) .filter(col(\"amount_of_records\") > 1) .drop(\"amount_of_records\") .distinct() ) for column in duplication_columns: q.append(f\"old.{column} = new.{column}\") q = \" AND \".join(q) # Remove all the duplicate records delta_table.alias(\"old\").merge( duplicate_records.alias(\"new\"), q ).whenMatchedDelete().execute()","title":"kill_duplicates()"},{"location":"reference/mack/#mack.latest_version","text":"Parameters: Name Type Description Default delta_table DeltaTable required Returns: Type Description float Source code in mack/__init__.py def latest_version(delta_table: DeltaTable) -> float: \"\"\" <description> :param delta_table: <description> :type delta_table: DeltaTable :returns: <description> :rtype: float \"\"\" version = delta_table.history().agg(max(\"version\")).collect()[0][0] return version","title":"latest_version()"},{"location":"reference/mack/#mack.rename_delta_table","text":"Renames a Delta table to a new name. This function can be used in a Databricks environment or with a standalone Spark session. Parameters: delta_table (DeltaTable): The DeltaTable object representing the table to be renamed. new_table_name (str): The new name for the table. table_location (str, optional): The file path where the table is stored. Defaults to None. If None, the function will attempt to determine the location from the DeltaTable object. databricks (bool, optional): A flag indicating whether the function is being run in a Databricks environment. Defaults to False. If True, a SparkSession must be provided. spark_session (pyspark.sql.SparkSession, optional): The Spark session. Defaults to None. Required if databricks is set to True. Returns: None Raises: TypeError: If the provided delta_table is not a DeltaTable object, or if databricks is True and spark_session is None. Example Usage: rename_delta_table(existing_delta_table, \"new_table_name\") Source code in mack/__init__.py def rename_delta_table( delta_table: DeltaTable, new_table_name: str, table_location: str = None, databricks: bool = False, spark_session: pyspark.sql.SparkSession = None, ) -> None: \"\"\" Renames a Delta table to a new name. This function can be used in a Databricks environment or with a standalone Spark session. Parameters: delta_table (DeltaTable): The DeltaTable object representing the table to be renamed. new_table_name (str): The new name for the table. table_location (str, optional): The file path where the table is stored. Defaults to None. If None, the function will attempt to determine the location from the DeltaTable object. databricks (bool, optional): A flag indicating whether the function is being run in a Databricks environment. Defaults to False. If True, a SparkSession must be provided. spark_session (pyspark.sql.SparkSession, optional): The Spark session. Defaults to None. Required if `databricks` is set to True. Returns: None Raises: TypeError: If the provided `delta_table` is not a DeltaTable object, or if `databricks` is True and `spark_session` is None. Example Usage: >>> rename_delta_table(existing_delta_table, \"new_table_name\") \"\"\" if not isinstance(delta_table, DeltaTable): raise TypeError(\"An existing delta table must be specified for delta_table.\") if databricks and spark_session is None: raise TypeError(\"A spark session must be specified for databricks.\") if databricks: spark_session.sql(f\"ALTER TABLE {delta_table.name} RENAME TO {new_table_name}\") else: delta_table.toDF().write.format(\"delta\").mode(\"overwrite\").saveAsTable( new_table_name )","title":"rename_delta_table()"},{"location":"reference/mack/#mack.show_delta_file_sizes","text":"Parameters: Name Type Description Default delta_table DeltaTable required humanize_binary bool False Returns: Type Description None Source code in mack/__init__.py def show_delta_file_sizes( delta_table: DeltaTable, humanize_binary: bool = False ) -> None: \"\"\" <description> :param delta_table: <description> :type delta_table: DeltaTable :param humanize_binary: <description> :type humanize_binary: bool :returns: <description> :rtype: None \"\"\" details = delta_table.detail().select(\"numFiles\", \"sizeInBytes\").collect()[0] size_in_bytes, number_of_files = details[\"sizeInBytes\"], details[\"numFiles\"] average_file_size_in_bytes = round(size_in_bytes / number_of_files, 0) if humanize_binary: humanized_size_in_bytes = humanize_bytes_binary(size_in_bytes) humanized_average_file_size = humanize_bytes_binary(average_file_size_in_bytes) else: humanized_size_in_bytes = humanize_bytes(size_in_bytes) humanized_average_file_size = humanize_bytes(average_file_size_in_bytes) humanized_number_of_files = f\"{number_of_files:,}\" print( f\"The delta table contains {humanized_number_of_files} files with a size of {humanized_size_in_bytes}.\" + f\" The average file size is {humanized_average_file_size}\" )","title":"show_delta_file_sizes()"},{"location":"reference/mack/#mack.type_2_scd_generic_upsert","text":"Parameters: Name Type Description Default delta_table DeltaTable DeltaTable required updates_df DataFrame required primary_key str required attr_col_names List [ str ] required is_current_col_name str required effective_time_col_name str required end_time_col_name str required Returns: Type Description None Raises: Type Description TypeError Raises type error when required column names are not in the base table. TypeError Raises type error when required column names for updates are not in the attributes columns list. Source code in mack/__init__.py def type_2_scd_generic_upsert( delta_table: DeltaTable, updates_df: DataFrame, primary_key: str, attr_col_names: List[str], is_current_col_name: str, effective_time_col_name: str, end_time_col_name: str, ) -> None: \"\"\" <description> :param delta_table: DeltaTable :type path: str :param updates_df: <description> :type updates_df: DataFrame :param primary_key: <description> :type primary_key: str :param attr_col_names: <description> :type attr_col_names: List[str] :param is_current_col_name: <description> :type is_current_col_name: str :param effective_time_col_name: <description> :type effective_time_col_name: str :param end_time_col_name: <description> :type effective_time_col_name: str :raises TypeError: Raises type error when required column names are not in the base table. :raises TypeError: Raises type error when required column names for updates are not in the attributes columns list. :returns: <description> :rtype: None \"\"\" # validate the existing Delta table base_col_names = delta_table.toDF().columns required_base_col_names = ( [primary_key] + attr_col_names + [is_current_col_name, effective_time_col_name, end_time_col_name] ) if sorted(base_col_names) != sorted(required_base_col_names): raise TypeError( f\"The base table has these columns {base_col_names!r}, but these columns are required {required_base_col_names!r}\" ) # validate the updates DataFrame updates_col_names = updates_df.columns required_updates_col_names = ( [primary_key] + attr_col_names + [effective_time_col_name] ) if sorted(updates_col_names) != sorted(required_updates_col_names): raise TypeError( f\"The updates DataFrame has these columns {updates_col_names!r}, but these columns are required {required_updates_col_names!r}\" ) # perform the upsert updates_attrs = list( map(lambda attr: f\"updates.{attr} <> base.{attr}\", attr_col_names) ) updates_attrs = \" OR \".join(updates_attrs) staged_updates_attrs = list( map(lambda attr: f\"staged_updates.{attr} <> base.{attr}\", attr_col_names) ) staged_updates_attrs = \" OR \".join(staged_updates_attrs) staged_part_1 = ( updates_df.alias(\"updates\") .join(delta_table.toDF().alias(\"base\"), primary_key) .where(f\"base.{is_current_col_name} = true AND ({updates_attrs})\") .selectExpr(\"NULL as mergeKey\", \"updates.*\") ) staged_part_2 = updates_df.selectExpr(f\"{primary_key} as mergeKey\", \"*\") staged_updates = staged_part_1.union(staged_part_2) thing = {} for attr in attr_col_names: thing[attr] = f\"staged_updates.{attr}\" thing2 = { primary_key: f\"staged_updates.{primary_key}\", is_current_col_name: \"true\", effective_time_col_name: f\"staged_updates.{effective_time_col_name}\", end_time_col_name: \"null\", } res_thing = {**thing, **thing2} res = ( delta_table.alias(\"base\") .merge( source=staged_updates.alias(\"staged_updates\"), condition=pyspark.sql.functions.expr(f\"base.{primary_key} = mergeKey\"), ) .whenMatchedUpdate( condition=f\"base.{is_current_col_name} = true AND ({staged_updates_attrs})\", set={ is_current_col_name: \"false\", end_time_col_name: f\"staged_updates.{effective_time_col_name}\", }, ) .whenNotMatchedInsert(values=res_thing) .execute() ) return res","title":"type_2_scd_generic_upsert()"},{"location":"reference/mack/#mack.type_2_scd_upsert","text":"Parameters: Name Type Description Default path DeltaTable required updates_df DataFrame required primary_key str required attr_col_names List [ str ] required Returns: Type Description None Source code in mack/__init__.py def type_2_scd_upsert( delta_table: DeltaTable, updates_df: DataFrame, primary_key: str, attr_col_names: List[str], ) -> None: \"\"\" <description> :param path: <description> :type path: DeltaTable :param updates_df: <description> :type updates_df: DataFrame :param primary_key: <description> :type primary_key: str :param attr_col_names: <description> :type attr_col_names: List[str] :returns: <description> :rtype: None \"\"\" return type_2_scd_generic_upsert( delta_table, updates_df, primary_key, attr_col_names, \"is_current\", \"effective_time\", \"end_time\", )","title":"type_2_scd_upsert()"},{"location":"reference/mack/#mack.validate_append","text":"Parameters: Name Type Description Default delta_table DeltaTable required append_df DataFrame required required_cols List [ str ] required optional_cols List [ str ] required Raises: Type Description TypeError Raises type error when input arguments have a invalid type, are missing or are empty. TypeError Raises type error when required columns are missing in the provided delta table. TypeError Raises type error when column in append dataframe is not part of the original delta table.. Source code in mack/__init__.py def validate_append( delta_table: DeltaTable, append_df: DataFrame, required_cols: List[str], optional_cols: List[str], ) -> None: \"\"\" <description> :param delta_table: <description> :type delta_table: DeltaTable :param append_df: <description> :type append_df: DataFrame :param required_cols: <description> :type required_cols: List[str] :param optional_cols: <description> :type optional_cols: List[str] :raises TypeError: Raises type error when input arguments have a invalid type, are missing or are empty. :raises TypeError: Raises type error when required columns are missing in the provided delta table. :raises TypeError: Raises type error when column in append dataframe is not part of the original delta table.. \"\"\" if not isinstance(delta_table, DeltaTable): raise TypeError(\"An existing delta table must be specified.\") if not isinstance(append_df, DataFrame): raise TypeError(\"You must provide a DataFrame that is to be appended.\") append_data_columns = append_df.columns for required_column in required_cols: if required_column not in append_data_columns: raise TypeError( f\"The base Delta table has these columns {append_data_columns!r}, but these columns are required {required_cols!r}\" ) table_columns = delta_table.toDF().columns for column in append_data_columns: if column not in table_columns and column not in optional_cols: raise TypeError( f\"The column {column!r} is not part of the current Delta table.\" + \" If you want to add the column to the table you must set the optional_cols parameter.\" ) details = delta_table.detail().select(\"location\").collect()[0] ( append_df.write.format(\"delta\") .mode(\"append\") .option(\"mergeSchema\", \"true\") .save(details[\"location\"]) )","title":"validate_append()"},{"location":"reference/mack/#mack.with_md5_cols","text":"Parameters: Name Type Description Default df Union [ DeltaTable , DataFrame ] required cols List [ str ] required output_col_name Optional [ str ] None Returns: Type Description DataFrame Raises: Type Description TypeError Raises type error when no composite key can be found. Source code in mack/__init__.py def with_md5_cols( df: Union[DeltaTable, DataFrame], cols: List[str], output_col_name: Optional[str] = None, ) -> DataFrame: \"\"\" <description> :param df: <description> :type df: DeltaTable or DataFrame :param cols: <description> :type cols: List[str] :param output_col_name: <description> :type output_col_name: str, defaults to empty string. :raises TypeError: Raises type error when no composite key can be found. :returns: <description> :rtype: DataFrame \"\"\" if output_col_name is None: output_col_name = \"_\".join([\"md5\"] + cols) if type(df) == DeltaTable: df = df.toDF() return df.withColumn(output_col_name, md5(concat_ws(\"||\", *cols)))","title":"with_md5_cols()"}]}